<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A deep dive into variational auto-encoder | Andrew Wong</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="A deep dive into variational auto-encoder" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An in-depth notes on VAE, with pytorch step-by-step explanation" />
<meta property="og:description" content="An in-depth notes on VAE, with pytorch step-by-step explanation" />
<link rel="canonical" href="ctawong.com/vae.html" />
<meta property="og:url" content="ctawong.com/vae.html" />
<meta property="og:site_name" content="Andrew Wong" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-01T00:00:00-04:00" />
<script type="application/ld+json">
{"headline":"A deep dive into variational auto-encoder","dateModified":"2020-04-01T00:00:00-04:00","datePublished":"2020-04-01T00:00:00-04:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"ctawong.com/vae.html"},"url":"ctawong.com/vae.html","description":"An in-depth notes on VAE, with pytorch step-by-step explanation","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="ctawong.com/feed.xml" title="Andrew Wong" /><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Andrew Wong</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/articles/">Articles</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A deep dive into variational auto-encoder</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-04-01T00:00:00-04:00" itemprop="datePublished">Apr 1, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This is a deep dive to maching learning technique <em>variational auto-encoder</em> (VAE).</p>

<ul id="markdown-toc">
  <li><a href="#part-1---general-theory" id="markdown-toc-part-1---general-theory">Part 1 - General Theory</a>    <ul>
      <li><a href="#model" id="markdown-toc-model">Model</a></li>
      <li><a href="#objective-function" id="markdown-toc-objective-function">Objective function</a></li>
    </ul>
  </li>
  <li><a href="#part-2---model-for-mnist-dataset" id="markdown-toc-part-2---model-for-mnist-dataset">Part 2 - Model for MNIST dataset</a></li>
  <li><a href="#part-3---pytorch-implementation" id="markdown-toc-part-3---pytorch-implementation">Part 3 - Pytorch implementation</a></li>
  <li><a href="#variational-class" id="markdown-toc-variational-class">Variational class</a>    <ul>
      <li><a href="#steps" id="markdown-toc-steps">Steps</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="part-1---general-theory">Part 1 - General Theory</h2>
<h3 id="model">Model</h3>
<p>The digit images (<script type="math/tex">x</script>) are generated by an unknown process <script type="math/tex">p_{\theta^*}(x\vert z)</script>, where <script type="math/tex">z</script> is an unobservable latent variable. Here we plug in a specific <script type="math/tex">z</script> into this function, and it specifies the intensity <em>distribution</em> of the image pixels of <script type="math/tex">x</script>.</p>

<p>We don’t know the true parameter <script type="math/tex">\theta^*</script> but we know the general form of the parametric function <script type="math/tex">p_{\theta}(x\vert z)</script>. Note that <script type="math/tex">\theta</script> denotes variable of parameters, while <script type="math/tex">\theta^*</script> denotes the truth value. Latent variable <script type="math/tex">z</script> follows distribution <script type="math/tex">p_{\theta^*}(z)</script>.</p>

<p>The <em>likelihood function</em> <script type="math/tex">p_{\theta}(x)</script> measures how well <script type="math/tex">\theta</script> describes the observed digit <script type="math/tex">x</script>.  By definition <script type="math/tex">p_{\theta^*}(x) = 1</script> because <script type="math/tex">\theta^*</script> generates the observation. However, <script type="math/tex">p_{\theta}(x)</script> is intractable, meaning that we cannot evaluate or differentiate <script type="math/tex">p_{\theta}(x)</script> for every <script type="math/tex">\theta</script> and <script type="math/tex">x</script>.  Same for <script type="math/tex">p_{\theta}(x\vert z)</script> and <script type="math/tex">p_{\theta}(z\vert x)</script>.  Since <script type="math/tex">p_{\theta}(z\vert x)</script> is hard to evaluate,  we introduce a new function <script type="math/tex">q_{\phi}(z\vert x)</script> to approximate it.  The following diagram sums it all up:</p>

<script src="https://cdn.jsdelivr.net/npm/mermaid@8.4.0/dist/mermaid.min.js"></script>
<div class="mermaid">
graph LR
  P("Draw from<br />distribution $P(z)$") --&gt; A
  A(("latent<br />variable $z$")) --&gt; H(Hidden<br />process )
  H --&gt;X((observed<br />image $x$))
  X--&gt;Q("$q(z\vert X)$")
  Q--&gt;P
  style H fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5, 5
</div>

<p>In summary, below are what each variable means.</p>

<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">x</script></td>
      <td>image of digit</td>
    </tr>
    <tr>
      <td><script type="math/tex">z</script></td>
      <td>latent variable that generates the image</td>
    </tr>
    <tr>
      <td><script type="math/tex">\theta</script></td>
      <td>Parameter of the true model  <script type="math/tex">p_{\theta}</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\theta^*</script></td>
      <td>True parameter. <script type="math/tex">p_{\theta^*}(x)=1</script> is maximum</td>
    </tr>
    <tr>
      <td><script type="math/tex">p_{\theta}(x)</script></td>
      <td>Likelihood function - How likely <script type="math/tex">\theta</script> describes <script type="math/tex">x</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">p_{\theta}(z)</script></td>
      <td>Prior distribution of <script type="math/tex">z</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">p_{\theta}(x\vert z)</script></td>
      <td>Likelihood or generative function of <script type="math/tex">x</script> given <script type="math/tex">z</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">p_{\theta}(z\vert x)</script></td>
      <td>Likelihood or generative function of <script type="math/tex">z</script> given <script type="math/tex">x</script></td>
    </tr>
    <tr>
      <td><script type="math/tex">q_{\phi}(z\vert x)</script></td>
      <td>Approximate function to <script type="math/tex">p_{\theta}(z\vert x)</script></td>
    </tr>
  </tbody>
</table>

<h3 id="objective-function">Objective function</h3>
<p>In variational auto encoder, the objective function is the likelihood function of the true model <script type="math/tex">p_{\theta}(x)</script>.  Ideally we want to find the optimal <script type="math/tex">\theta</script> that best matches the observed images <script type="math/tex">x</script>.  <script type="math/tex">p_{\theta}(x)</script> can be expressed in terms of Kullback-Leiber divergence <script type="math/tex">D_{KL}</script>.  <script type="math/tex">D_{KL}(q_{\phi}(z\vert x) ||  p_{\theta(z\vert x) })</script> measures how well <script type="math/tex">q_{\phi}(z\vert x)</script> approximates  <script type="math/tex">p_{\theta(z\vert x)}</script>:</p>

<script type="math/tex; mode=display">D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta(z\vert x) })  = \mathbb{E}_{q_{\phi}(z\vert x)}[\ln q_{\phi}(z\vert x)-\ln p(x,z) + \ln p_{\theta}(x)]</script>

<p>or</p>

<script type="math/tex; mode=display">\ln p_{\theta}(x) = D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta}(z\vert x) )  + \mathcal{L}.</script>

<p>where</p>

<script type="math/tex; mode=display">\mathcal{L} = \mathbb{E}_{q_{\phi}(z\vert x)}[-\ln q_{\phi}(z\vert x)+\ln p_{\theta}(x,z) ]</script>

<p><script type="math/tex">\ln p_{\theta}(x)</script> can be taken out of the expectation because it does not depend on <script type="math/tex">z</script>. Since D_{KL} is non-negative, <script type="math/tex">\mathcal{L}</script> serves as a <em>lower bound</em> to <script type="math/tex">\ln p_{\theta}(x)</script>. In other words:</p>

<script type="math/tex; mode=display">\ln p_{\theta}(x)  \ge \mathcal{L}</script>

<p>The trick of VAE is to  maximize <script type="math/tex">\mathcal{L}</script> instead of <script type="math/tex">\ln p_{\theta}(x)</script> because it can be calculated for many problems.</p>

<p>But since <script type="math/tex">p(x,y)</script> is hard to calculate, it is useful to rewrite <script type="math/tex">\mathcal{L}</script> as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}
\mathcal{L} &= \mathbb{E}_{q_{\phi}(z\vert x)}[-\ln q_{\phi}(z\vert x)+\ln p_{\theta}(x,z) ]\\
            &= \mathbb{E}_{q_{\phi}(z\vert x)}[-\ln q_{\phi}(z\vert x)+\ln p_{\theta}(z)+\ln p_{\theta}(x|z) ]\\
            &= -D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) + \mathbb{E}_{q_{\phi}(z\vert x)} [\ln p_{\theta}(x|z)]
\end{array} %]]></script>

<h2 id="part-2---model-for-mnist-dataset">Part 2 - Model for MNIST dataset</h2>
<p>To trian the model, we will maximize the lower bound</p>

<script type="math/tex; mode=display">\mathcal{L} = -D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) + \mathbb{E}_{q_{\phi}(z\vert x)} [\ln p_{\theta}(x|z)]</script>

<p>For this problem, we choose the prior distribution <script type="math/tex">p_{\theta}(z)</script> of the latent variable <script type="math/tex">z</script> to be the standard normal distribution which has zero mean and unit variance, i.e. <script type="math/tex">\mathcal{N}(0,1)</script>. Why can we do that? Because we don’t know the distribution and may as well choose to work with a easier one! But we will see in a moment that for the purpose it will serve it doesn’t really matter.</p>

<p>OK, that takes care of <script type="math/tex">p_{\theta}(z)</script>. How about <script type="math/tex">q_{\phi}(z\vert x)</script>? To be consistent, we also model it with a normalize distribution, but it could have non-zero mean and non-unity variance. Mathematically,</p>

<script type="math/tex; mode=display">q_{\phi}(z\vert x) = \mathcal{N}(\mu, \sigma^2)</script>

<p>How can it be different from the prior? The idea is, we have 10 digits to encode. Each one will have distribution deviates from zero and collectively they form 10 distinct clusters. But if we look at $z$ over all digits, it will still follow the standard normal distribution <script type="math/tex">\mathcal{N}(0,1)</script>. In a moment, you will see the prior <script type="math/tex">p_{\theta}(z)</script> regularizes the learned parameters <script type="math/tex">\mu</script> and <script type="math/tex">\sigma</script>  to pull them back to the standard normal <script type="math/tex">\mathcal{N}(0,1)</script>.</p>

<p>Now we can calculate the first term, <script type="math/tex">D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z))</script>. Using the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions">identity</a> of KL divergence between two normal distributions</p>

<script type="math/tex; mode=display">D_{KL}(\mathcal{N}(\mu_1, \sigma_1^2)\vert\vert \mathcal{N}(\mu_2, \sigma_2^2)) =  \ln \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2 \sigma_2^2} - \frac{1}{2}</script>

<p>We get the first term</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}
--D_{KL}(q_{\phi}(z\vert x) \vert \vert   p_{\theta } (z)) &= -D_{KL}(\mathcal{N}(\mu, \sigma^2) \vert \vert  \mathcal{N}(0, 1) )\\
                       &= \frac{1}{2} \sum_\limits{j=1}^J(1 + \ln \sigma_j^2 -\sigma_j^2 - \mu_j^2)
\end{array} %]]></script>

<p>$J$ is the dimension of the latent variable $z$.</p>

<p>Let’s build some intuition!</p>
<ul>
  <li>$\ln \sigma_j^2 -\sigma_j^2$ is maximized at $\sigma_j^2 = 1$ (<a href="https://www.wolframalpha.com/input/?i=plot+ln+%28sigma%5E2%29+-+sigma%5E2+from+0+to+5">plot</a>)</li>
  <li>$-\mu_j^2$ is maximized when $\mu_j = 0$ (hope this is obvious…)</li>
</ul>

<p>So the learning prefers $z$ to follow the standard normal distribution as much as possible. In other words, the first term is a regularization term to make sure that the learned distribution of $z$ is not too crazy.</p>

<p>OK, let’s understand the second term <script type="math/tex">\mathbb{E}_{q_{\phi}(z\vert x)} [\ln p_{\theta}(x\vert z)]</script>. For binary images (we will use binarized MNIST),  $p_{\theta}(x\vert z)$ is Bernoulli distribution . For a single pixel, it is</p>

<script type="math/tex; mode=display">p_{\theta}(x|z) =x^y(1-x)^{(1-y)}</script>

<p>$y$ is the observed pixel intensity and can only be 0 or 1 (binary).  Basically, this is a measure of how well the random variable $x$ matches the observed intensity. Suppose it is a dark pixel ($y=0$) and the model predicts $x=0.1$,  the model would be considered to be doing pretty well and score 0.9 (with 1.0 being the full mark). Note that you don’t see $z$ on right hand side of the equation because it is  implicitly in $x$ as $x$ is generated by $z$.   For an image with $L$ pixels,</p>

<script type="math/tex; mode=display">p_{\theta}(x|z) =\prod\limits_{i=1}^{L} x_i^{y_i}(1-x_i)^{(1-y_i)}</script>

<p>How to evaluate the expectation value <script type="math/tex">\mathbb{E}_{q_{\phi}(z\vert x)}</script>?  Since we draw $z$ from the approximate $q_{\phi}(z\vert x)$, we can take an simple average over all images, or use only one image per evaluation and don’t worry about it.</p>

<h2 id="part-3---pytorch-implementation">Part 3 - Pytorch implementation</h2>

<p><code class="highlighter-rouge">model</code> takes latent variable <script type="math/tex">z</script>  and generates digit <script type="math/tex">x</script></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""Bernoulli model parameterized by a generative network with Gaussian latents for MNIST."""</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_size</span><span class="p">,</span> <span class="n">data_size</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'p_z_loc'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">latent_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'p_z_scale'</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">latent_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_p_z</span> <span class="o">=</span> <span class="n">NormalLogProb</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_p_x</span> <span class="o">=</span> <span class="n">BernoulliLogProb</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">generative_network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">latent_size</span><span class="p">,</span>
                                            <span class="n">output_size</span><span class="o">=</span><span class="n">data_size</span><span class="p">,</span> 
                                            <span class="n">hidden_size</span><span class="o">=</span><span class="n">latent_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>
<p><script type="math/tex">x</script>: digit image (size: 28x28 = 784)
<script type="math/tex">z</script>: latent variable (size: 128)
<code class="highlighter-rouge">log_p_z</code>:  <script type="math/tex">\log p(z)</script> is log normal distribution with zero mean and unit variance.<br />
<code class="highlighter-rouge">log_p_x</code>: <script type="math/tex">\log p(x)</script> is log Bernoulli distribution with logits, i.e. <script type="math/tex">y\log \sigma(x) + (1-y)\log[1-\sigma(x)]</script>, <script type="math/tex">\sigma</script> is the Sigmoid function. <script type="math/tex">y</script> is probability of a bright pixel.
<code class="highlighter-rouge">generative_network</code> takes  latent variable <script type="math/tex">z</script> and generates a digit image <script type="math/tex">x</script>. Note that need to pass x through a Sigmoid function (i.e. <script type="math/tex">\sigma(x)</script> ) to form a digit with intensity range 0 to 1.</p>

<p>### Feed forward of Model</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="s">"""Return log probability of model.
       Generates logits from z, then compare with truth x
    """</span>
    <span class="n">log_p_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_p_z</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p_z_loc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_z_scale</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generative_network</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="c1"># unsqueeze sample dimension
</span>    <span class="n">logits</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">log_p_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_p_x</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># note that this is function of both generated logits and truth x
</span>    <span class="k">return</span> <span class="n">log_p_z</span> <span class="o">+</span> <span class="n">log_p_x</span>
</code></pre></div></div>
<p><script type="math/tex">\log p(z)</script> is the prior probability of observing <script type="math/tex">z</script>.  It already sums over the dimension of <script type="math/tex">z</script>.  We can do that because <script type="math/tex">p(z) = p(z_1)p(z_2)...p(z_n)</script> for normal distribution.  So we have one value for each z.<br />
<code class="highlighter-rouge">logits</code>  <script type="math/tex">x_z</script> is generated by the model using <script type="math/tex">z</script>. It is the generated digit image.</p>

<p><script type="math/tex">\log p( \sigma(x_z), x )</script> is function that compare <script type="math/tex">\sigma(x_z)</script> and <script type="math/tex">x</script>.</p>

<ul>
  <li>
    <script type="math/tex; mode=display">p(0,0) = 0</script>
  </li>
  <li>
    <script type="math/tex; mode=display">p(1,1) = 0</script>
  </li>
  <li>
    <script type="math/tex; mode=display">p(-1 ,1) = -27.6</script>
  </li>
  <li>
    <script type="math/tex; mode=display">p(1 ,-1) = -27.6</script>
  </li>
</ul>

<p>for each digit.  So <script type="math/tex">\log p( \sigma(x_z), x )</script> is maximized when the generated digit matches the truth. Note <code class="highlighter-rouge">log_p_x</code> is  <script type="math/tex">\log p( \sigma(x_z), x )</script> with negative sign included.</p>

<p>The <code class="highlighter-rouge">forward</code> function returns <script type="math/tex">p_{xz}(x,z)</script>, the joint prior probability of <script type="math/tex">x</script> and <script type="math/tex">z</script>.  Using <script type="math/tex">\log p_{xz}(x,z) = \log p_x(x) p_z(z) = \log p_x(x) + \log p_z(z)</script>, the <code class="highlighter-rouge">forward</code> function returns <script type="math/tex">\log p_z(z) + \log p( \sigma(x_z),x )</script>.  It has dimension of the batch size (128).</p>
<h2 id="variational-class">Variational class</h2>

<p><code class="highlighter-rouge">VariationalMeanField</code> takes an image <script type="math/tex">x</script> and returns the latent variable <script type="math/tex">z</script></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VariationalMeanField</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""Approximate posterior parameterized by an inference network."""</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_size</span><span class="p">,</span> <span class="n">data_size</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inference_network</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">data_size</span><span class="p">,</span> 
                                           <span class="n">output_size</span><span class="o">=</span><span class="n">latent_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> 
                                           <span class="n">hidden_size</span><span class="o">=</span><span class="n">latent_size</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_q_z</span> <span class="o">=</span> <span class="n">NormalLogProb</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
</code></pre></div></div>

<p><a href="https://pytorch.org/docs/stable/nn.html#softplus">Softplus</a> is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="s">"""Return sample of latent variable and log prob."""</span>
    <span class="n">loc</span><span class="p">,</span> <span class="n">scale_arg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inference_network</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Infer parameters of z from x (loc and scale)
</span>    <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">scale_arg</span><span class="p">)</span> <span class="c1"># make sure scale is positive
</span>    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">loc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">loc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">loc</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">loc</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">eps</span>  <span class="c1"># reparameterization
</span>    <span class="n">log_q_z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_q_z</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">log_q_z</span>
</code></pre></div></div>
<p><code class="highlighter-rouge">inference_network</code> returns a vector two times of size of latent variable.  It takes a digit image <script type="math/tex">x</script> as input and output the <em>parameters</em> of probability distribution of latent variable <script type="math/tex">z</script>. i.e. mean (<code class="highlighter-rouge">loc</code>) and variance (<code class="highlighter-rouge">scale</code>)  which have dimension of latent variable <script type="math/tex">z</script>.</p>

<p>The reparameterization trick is to redraw the latent variable <script type="math/tex">z</script> from normal distribution with parameters (i.e. <code class="highlighter-rouge">loc</code> and <code class="highlighter-rouge">scale</code>) inferred from the original <script type="math/tex">z</script>.  This has effect of increasing the variance of <script type="math/tex">z</script>.</p>

<p><script type="math/tex">\log q(z)</script> (<code class="highlighter-rouge">log_q_z</code>) is the log probability of the new <script type="math/tex">z</script>, one value per image.</p>

<h3 id="steps">Steps</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cycle</span><span class="p">(</span><span class="n">train_data</span><span class="p">)):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">variational</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span><span class="p">,</span> <span class="n">log_q_z</span> <span class="o">=</span> <span class="n">variational</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">log_p_x_and_z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="c1"># average over sample dimension
</span>    <span class="n">elbo</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_p_x_and_z</span> <span class="o">-</span> <span class="n">log_q_z</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># sum over batch dimension
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">elbo</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<ol>
  <li>Infer latent variable <script type="math/tex">z</script> from <script type="math/tex">x</script>. Return the reparameterized <script type="math/tex">z</script> and its log probability <script type="math/tex">\log q(z)</script></li>
  <li>Generative model (<code class="highlighter-rouge">model</code>)  uses reparameterized <script type="math/tex">z</script>  to get  <script type="math/tex">\log p(z) - \log p( \sigma(x_z), x )</script> (<code class="highlighter-rouge">log_p_x_and_z</code>)</li>
  <li>ELBO = <script type="math/tex">\log p(z) +\log p( \sigma(x_z),x ) - \log q(z)</script></li>
</ol>

<h2 id="references">References</h2>

<p><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a> - original paper</p>


  </div><a class="u-url" href="/vae.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Andrew Wong</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Andrew Wong</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/ctawong"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ctawong</span></a></li><li><a href="https://www.twitter.com/ctawongml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">ctawongml</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>machine learning, physics, biotechnology</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
